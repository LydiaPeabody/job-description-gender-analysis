{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import requests\n",
    "import re \n",
    "\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "from selenium import webdriver\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy for collection\n",
    "\n",
    "1. Pull job links from a search site\n",
    "2. Iterate through collected links to gather data on specific jobs from indivual pages:\n",
    "    * job title\n",
    "    * company \n",
    "    * company rating\n",
    "    * location\n",
    "    * salary (if provided)\n",
    "    * brief description\n",
    "\n",
    "Query url follow a simple format, so we can easily generate search queries. For example a search for \"data scientist\" in Melbourne, Vic is: https://au.indeed.com/jobs?q=data+scientist&l=Melbourne+VIC <br>\n",
    "\n",
    "\n",
    "### These functions allow us to do the basic data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing both selenium and request version to allow switching\n",
    "\n",
    "def make_fresh_soup(url):\n",
    "    '''Accepts a url and returns a BeautifulSoup object'''\n",
    "    \n",
    "    #testing just using request\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    \n",
    "#     #use driver and wait one second for javascript to run before capturing html\n",
    "#     driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver.exe\")\n",
    "#     driver.get(url)\n",
    "#     sleep(1)\n",
    "#     html = driver.page_source     \n",
    "    \n",
    "    #convert to soup object\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "#     #close the driver to keep things clean\n",
    "#     driver.close()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indeed_search(search_terms, location=['Melbourne','VIC']):\n",
    "    \n",
    "    '''Function accepts lists of job search terms and optional location strings \n",
    "    Returns the url for and indeeed search\n",
    "    Search_terms should be a list of terms e.g.['data','scientist']\n",
    "    Location should be a list of city, state; default is ['Melbourne','VIC']'''\n",
    "    \n",
    "    #setting and formatting terms for search\n",
    "    search_string = search_terms[0]\n",
    "    for term in search_terms[1:]:\n",
    "        search_string = search_string + '+' + term\n",
    "\n",
    "    #adding location if provided\n",
    "    location_string = location[0] + '+' + location[1]\n",
    "    search_string = search_string + '&l=' + location_string\n",
    "        \n",
    "    #setting url for scraping\n",
    "    search_url = 'https://au.indeed.com/jobs?q=' + search_string\n",
    "\n",
    "\n",
    "\n",
    "    return(search_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_links(soup_search):\n",
    "    \n",
    "    '''Function to collect the links from an indeed search page\n",
    "    Accepts a BeautifulSoup object of an indeed search page as input\n",
    "    Returns a list of all the links to jobs on the page'''\n",
    "    \n",
    "    #adds website root to collected page elements\n",
    "    links = [('https://au.indeed.com' + x.get('href')) \n",
    "                 for x in soup_search.find_all('a', attrs={'data-tn-element':'jobTitle'})]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_link(soup_search):\n",
    "    \n",
    "    '''Function to pull out the link to the next page of jobs for the search\n",
    "    Accepts a BeautifulSoup object of the search page as input\n",
    "    Returns the link to the next results page, or None on the last page'''\n",
    "    \n",
    "    #find the last of the links to new pages\n",
    "    last_page_link = soup_search.find('div',{'class':'pagination'}).find_all('a')[-1]\n",
    "\n",
    "    #if the text for that link is Next, grab the link\n",
    "    if last_page_link.text.strip().startswith('Next') == True: \n",
    "        next_link = 'https://au.indeed.com' + last_page_link.get('href')\n",
    "    else:\n",
    "        next_link = 'end'\n",
    "\n",
    "\n",
    "    return next_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_search(url, links_list, count=1):\n",
    "    \n",
    "    '''Finds and returns a list of job pages found in a particular search\n",
    "    Accepts a search url and a list to collect the links from that search'''\n",
    "    \n",
    "    #including error handling to return result even if next page is not found\n",
    "    try:\n",
    "        original_links = links_list\n",
    "\n",
    "        #make soup object for url\n",
    "        soup_search = make_fresh_soup(url)\n",
    "        count += 1\n",
    "\n",
    "        #save the links from the soup object\n",
    "        page_links = get_job_links(soup_search)\n",
    "        all_links = original_links + page_links\n",
    "\n",
    "        #find the link to the next page\n",
    "        next_url = get_next_link(soup_search)\n",
    "\n",
    "        #repeat with next link until last page located\n",
    "        if next_url == 'end':\n",
    "            print(str(count) + 'pages of jobs searched')\n",
    "            return all_links\n",
    "\n",
    "        else:    \n",
    "            return do_search(next_url, all_links, count)\n",
    "    \n",
    "    except:\n",
    "        return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_details(job_url):\n",
    "    \n",
    "    '''Extracts the job title, company, company rating, \n",
    "    job description, and salary from a job listing'''\n",
    "    \n",
    "    #use selenium driver to freeze javascript and capture html\n",
    "    soup_job = make_fresh_soup(job_url)\n",
    "    \n",
    "    job_details = {}\n",
    "    \n",
    "    #these elements are almost always present, but adding exception management just in case\n",
    "    try:\n",
    "        job_details['job_title'] = soup_job.find('h3').text.strip()\n",
    "    except: \n",
    "        job_details['job_title'] = None\n",
    "\n",
    "    try:\n",
    "        job_details['company'] = soup_job.find('div', {'class':\"icl-u-lg-mr--sm icl-u-xs-mr--xs\"}).text.strip()\n",
    "    except:\n",
    "        job_details['company'] = None\n",
    "          \n",
    "    try:\n",
    "        job_details['location'] = soup_job.find('div', {'class':\"location\"}).text.strip()\n",
    "    except:\n",
    "        job_details['location'] = None\n",
    "        \n",
    "    try:\n",
    "        job_details['job_description_all_text'] = soup_job.find('div',{'id':'jobDescriptionText'}).text.strip()\n",
    "    except:\n",
    "        job_details['job_description_all_text'] = None\n",
    "\n",
    "    #formatting is slightly inconsistent on ratings\n",
    "    try:\n",
    "        job_details['company_rating'] = float(soup_job.find('div', {'class':\"icl-Ratings-starsCountWrapper\"})\n",
    "                                              .get('aria-label')[0:3])\n",
    "    except:\n",
    "        try:\n",
    "            job_details['company_rating'] = (soup_job.find('div', {'class':\"icl-Ratings-starsCountWrapper\"})\n",
    "                                         .get('aria-label')[0:3])\n",
    "        except:\n",
    "            job_details['company_rating'] = None\n",
    "            \n",
    "    #salary data is often missing\n",
    "    try:\n",
    "        job_details['salary_data_text'] = soup_job.find('span', {'class':\"icl-u-xs-mr--xs\"}).text.strip()\n",
    "    except:\n",
    "        job_details['salary_data_text'] = None\n",
    "\n",
    "    return(job_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collect_and_save_job_details(job_links, industry, location, filename):\n",
    "    \n",
    "    \"\"\"Iterates through a list of links for individual jobs\n",
    "    Collects the details of each job\n",
    "    Converts to dataframe and saves to csv\"\"\"\n",
    "    \n",
    "    file_path = './data/' + filename\n",
    "    jobs = []\n",
    "\n",
    "    for job in job_links:\n",
    "        new_job = get_job_details(job)\n",
    "        jobs.append(new_job)\n",
    "\n",
    "    jobs_df = pd.DataFrame(jobs)\n",
    "    jobs_df['industry'] = industry\n",
    "    jobs_df['location'] = location\n",
    "    jobs_df.to_csv(file_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def industry_location_search(industries, locations):\n",
    "    \n",
    "    '''Collects job details from and Indeed search including search terms and location\n",
    "    industries must be a list of lists of search terms (single terms must be a single list)\n",
    "    locations must be a list of [city, state] e.g. ['Melbourne', 'VIC']'''\n",
    "    \n",
    "    for industry in industries:\n",
    "        \n",
    "        for location in locations:\n",
    "            \n",
    "            job_links = []\n",
    "            \n",
    "            search_url = indeed_search(industry, location)\n",
    "            job_links = do_search(search_url, job_links)\n",
    "        \n",
    "            location_string = location[0].lower()\n",
    "            industry_string = industry[0]\n",
    "            for term in industry[1:]:\n",
    "                industry_string = industry_string + term\n",
    "    \n",
    "            filename = industry_string + '_' + location_string + '.csv' \n",
    "        \n",
    "            collect_and_save_job_details(job_links, industry_string, location_string, filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to retrieve and combine data from specific industries & locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(industries, locations):\n",
    "    \n",
    "    '''Combines previously saved .csv files for a location and industry into one file\n",
    "    industries and locations need to be from the lists of lists used for searching'''\n",
    "    \n",
    "    filenames = []\n",
    "    \n",
    "    for industry in industries:\n",
    "        \n",
    "        for location in locations:\n",
    "                    \n",
    "            location_string = location[0].lower()\n",
    "            \n",
    "            industry_string = industry[0]\n",
    "            for term in industry[1:]:\n",
    "                industry_string = industry_string + term\n",
    "    \n",
    "            filename = './data/' + industry_string + '_' + location_string + '.csv' \n",
    "            filenames.append(filename)\n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(industries, locations, final_file_path = './data/new_combined_data.csv'):\n",
    "    \n",
    "    '''Combines previously saved .csv files for a location and industry into one file\n",
    "    industries and locations need to be from the lists of lists used for searching.\n",
    "    Name for filepath to save combined csv is optional.\n",
    "    Saves a csv file and returns a dataframe of combined data'''\n",
    "    \n",
    "    frames = []\n",
    "    \n",
    "    for industry in industries:\n",
    "        \n",
    "        for location in locations:\n",
    "                    \n",
    "            location_string = location[0].lower()\n",
    "            \n",
    "            industry_string = industry[0]\n",
    "            for term in industry[1:]:\n",
    "                industry_string = industry_string + term\n",
    "    \n",
    "            filename = './data/' + industry_string + '_' + location_string + '.csv' \n",
    "            \n",
    "            df = pd.read_csv(filename)\n",
    "            frames.append(df)\n",
    "    \n",
    "    full_dataset = pd.concat(frames)\n",
    "    full_dataset.to_csv(final_file_path, index=False)\n",
    "        \n",
    "    return(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collected data for melbourne previously\n",
    "#collecting more location data\n",
    "industries = [['data', 'science'], ['data', 'analyst'], ['construction'], ['hospitality'], \n",
    "              ['nursing'], ['manufacturing'], ['retail'], ['finance'],['early','learning']]\n",
    "locations = [['Sydney', 'NSW'], ['Perth','WA'], ['Adelaide','SA'],\n",
    "             ['Canberra','ACT'], ['Brisbane','QLD'], ['Darwin','NT']]\n",
    "\n",
    "industry_location_search(industries,locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28156, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining all industries\n",
    "industries = [['data', 'science'], ['data', 'analyst'], ['construction'], ['hospitality'], \n",
    "              ['nursing'], ['manufacturing'], ['retail'], ['finance'],['early','learning']]\n",
    "locations = [['Sydney', 'NSW'], ['Perth','WA'], ['Adelaide','SA'],\n",
    "             ['Canberra','ACT'], ['Brisbane','QLD'], ['Darwin','NT']]\n",
    "\n",
    "df = combine_files(industries, locations)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4643, 7)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pulling out just data jobs for project 4\n",
    "industries = [['data', 'science'], ['data', 'analyst']]\n",
    "locations = [['Sydney', 'NSW'], ['Perth','WA'], ['Adelaide','SA'],\n",
    "             ['Canberra','ACT'], ['Brisbane','QLD'], ['Darwin','NT']]\n",
    "\n",
    "df = combine_files(industries, locations)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options for improvement: \n",
    "* Use the suggested searches at the bottom of the final page\n",
    "    * Continue until?? Some arbitrary level of data reached\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
